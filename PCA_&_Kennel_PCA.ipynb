{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PCA & Kennel PCA.ipynb",
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "authorship_tag": "ABX9TyP1hlcnIubwHr1X/hXLKtlO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghuilee91/Deeplearning/blob/master/PCA_%26_Kennel_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFiujov7S-KI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "import sys\n",
        "sys.path.append(\"/content/gdrive/My Drive/Colab Notebooks\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSGakuCZTCyE",
        "colab_type": "text"
      },
      "source": [
        "# PCA (주성분 분석, Principal Component Analysis)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "- 정의 : 고차원의 데이터를 저차원의 데이터로 환원시키는 기법\n",
        "- 필요 : 다차원의 데이터 분포를 가장 잘 표현하는 성분들을 찾아주기 위함\n",
        "- 특징 : <br>\n",
        "  a. 주성분의 차원수는 원래 표본의 차원수보다 작거나 같음<br>\n",
        "  b. 데이터를 한개의 축으로 투영시켰을 때 그 분산이 가장 커지는 축이 첫 번째 주성분(PC1)<br>\n",
        "  c. 두 번째로 커지는 축을 두 번째 주성분으로 놓이도록 새로운 좌표계로 데이터를 선형 변환시킴"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_rr0s0ljNJn",
        "colab_type": "text"
      },
      "source": [
        "## 1. 차원 축소 (Dimensionality reduction)\n",
        "*데이터에서 불필요한 feature를 제거하는 작업*\n",
        "\n",
        "> 대부분의 경우, 현실 세계의 문제는 가공되지 않은 데이터를 처리해야 한다. 예를 들어, 머신 러닝 모델을 이용하여 증명사진에 있는 인물의 성별을 맞추는 문제가 있을 때, 이 문제를 풀기 위해 우리는 성별이 표시된 증명사진을 머신 러닝 모델의 학습 데이터로 이용할 것이다. 하나의 사진이 **200X200의 이미지라고 하면, 해당 사진은 총 40,000개의 feature를 갖는 벡터로 표현**이 될 것이다. 그러나 대부분의 머신 러닝 모델은 입력 데이터의 차원이 클 경우, **차원의 저주와 학습 속도가 저하되는 문제**를 갖고 있다. 이를 위해 생각해볼 수 있는 것은 이미지에서 인물에 대한 정보를 포함하지 않는 부분을 제거하여 입력 데이터의 차원을 낮추는 것이다. 예를 들어, 증명사진의 왼쪽과 오른쪽 상단은 단색의 배경이기 때문에 인물에 대한 정보를 포함하지 않으며, 배경 부분을 제거하여 모델을 학습하여도 성능에는 큰 차이가 없을 것이다. 이와 같이 **데이터에서 불필요한 feature를 제거하는 작업을 차원 축소라고 한다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1ysLz8GjNbk",
        "colab_type": "text"
      },
      "source": [
        "## 2. 특징 선택 (Feature selection)과 특징 추출 (feature extraction)\n",
        "*차원 축소의 대표적 2가지 방법 : 특징 선택과 특징 추출*\n",
        "\n",
        "> 데이터의 차원을 축소하기 위한 방법으로는 크게 특징 선택 (feature selection)과 특징 추출 (feature extraction)이 있다.\n",
        "> * 특징 선택 : d 차원의 데이터 x를 구성하는 각 feature e1,e2,...,ed 중에서 어떠한 기준을 만족하는 p개의 feature를 선택한다. \n",
        "* 특징 추출:  기존의 feature e1,e2,...,ed를 조합하여 새로운 feature z1,z2,...,zp를 생성한다.\n",
        "\n",
        "<IMG SRC=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile1.uf.tistory.com%2Fimage%2F99EABC3E5B8A48781BEFA4\">\n",
        "\n",
        "\n",
        "- PCA는 Feature Extraction 임\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGK8nojQjNeo",
        "colab_type": "text"
      },
      "source": [
        "## 3. 주성분 분석 (PCA)\n",
        "\n",
        "- 주성분 분석은 가장 대표적인 차원 축소 알고리즘이다. PCA는 먼저 데이터에 가장 가까운 초평면(hyperplane)을 구한 다음, 데이터를 이 초평면에 투영(projection)시킨다.\n",
        "- 저차원의 초평면에 데이터를 투영하기 전에 먼저 적절한 초평면을 선택해야 한다. **PCA는 데이터의 분산이 최대가 되는 축을 찾는다. 즉, 원본 데이터셋과 투영된 데이터셋 간의 평균제곱거리를 최소화 하는 축**을 찾는다. 아래의 그림에서 왼쪽 2차원 데이터셋을 오른쪽 그림처럼 투영했을 때 ​축으로 투영한 데이터가 분산이 최대로 보존되는 것을 확인할 수 있다.\n",
        "\n",
        "\n",
        "*분산이 크다는건, 해당하는 변수에 따라 데이터가 뭔가 잘 흩어진다는 뜻이므로, 제일 설명력이 높다고 볼수 있다.*\n",
        "\n",
        "<IMG SRC=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile28.uf.tistory.com%2Fimage%2F99AC093E5B8A4904213CC3\">\n",
        "\n",
        "\n",
        "\n",
        "PCA에서 분산을 최대화하는 새로운 feature (축)를 찾는 이유는 다음의 예제를 보면 명확히 이해할 수 있다. 아래의 [그림 1]과 같이 두 개의 class C1,C2로 구성되며, 2차원 공간에 존재하는 데이터 X가 있다.\n",
        "\n",
        "<IMG SRC=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile6.uf.tistory.com%2Fimage%2F997EAF475B2F8EBC139455\">\n",
        "[그림 1] 예제 데이터의 분포\n",
        "\n",
        "2차원 공간에 존재하는 X를 1차원으로 차원 축소를 해야 하는 상황에서 우리는 데이터를 단순히 x-축으로 사상하는 방법을 생각해볼 수 있다. 만약 X를 x-축으로 사상한다면, 데이터는 1차원 feature space에서 [그림 2]와 같이 분포할 것이다.\n",
        "\n",
        "<IMG SRC=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile10.uf.tistory.com%2Fimage%2F99ACC33A5B2FA30E1FA9F6\">\n",
        "[그림 2] 예제 데이터에 대한 x-축 사상\n",
        "\n",
        "[그림 2]에서 A와 B에 해당하는 영역은 각각 C1과 C2에 속하는 데이터가 x-축에 사상되는 영역을 나타내며, C에 해당하는 영역은 C1과 C2의 데이터가 겹쳐지는 영역을 의미한다. Classification 또는 clustering의 관점에서 차원 축소 문제를 볼 때, C에 해당하는 영역이 클수록 classification 또는 clustering 알고리즘의 정확도는 하락할 것이다. 반면에 **C에 해당하는 영역이 아예 존재하지 않는다면 100%의 정확도를 갖는 classification 또는 clustering 알고리즘을 만드는 것은 그리 어려운 일이 아닐 것이다. 따라서, C 영역을 최소화하도록 차원 축소를 수행**하는 것은 매우 타당한 선택 중 하나이며, 이 목표는 데이터의 분산을 최대화하도록 차원 축소를 진행함으로써 간접적으로 달성할 수 있다. 이러한 관점에서 PCA는 [그림 3]의 z-축과 같은 주성분 (principal component)를 찾는다.\n",
        "\n",
        "<IMG SRC=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile10.uf.tistory.com%2Fimage%2F99AAA93B5B2F92781C2B41\">\n",
        "[그림 3] 예제 데이터에 대한 주성분, z-축 사상\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZgzjp7njNnx",
        "colab_type": "text"
      },
      "source": [
        "## 특징과 한계\n",
        "\n",
        "- PCA는 매우 넓게 사용된다. 이것은 어떠한 분포적인 가정을 요구하지 않는다. \n",
        "\n",
        "- 가장 큰 분산의 방향들이 가장 흥미로운 것으로 가정된다. \n",
        "\n",
        "- PCA는 오직 본래 변수들의 선형 결합을 고려한다. Kernel PCA는 PCA의 확장으로 non-linear mapping에 사용될 수 있다. \n",
        "\n",
        "- 차원 축소는 오직 본래 변수들이 상관이 있을 때에만 가능하다. 그렇지 않으면 PCA는 아무것도 하지 못한다. 그것들의 분산으로 다시 ordering 하는 것을 제외하면 말이다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3gq0IOw0M2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# 일관된 출력을 위해 유사난수 초기화\n",
        "np.random.seed(42)\n",
        "\n",
        "import pandas.util.testing as tm\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "\n",
        "m = 60\n",
        "w1, w2 = 0.1, 0.3\n",
        "noise = 0.1\n",
        "\n",
        "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
        "X = np.empty((m, 3))\n",
        "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
        "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
        "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)\n",
        "\n",
        "print('X.shape:', X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9HzTRh40aO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(X, columns=['$X_1$', '$X_2$', '$X_3$'])\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NUM-APW0mxu",
        "colab_type": "text"
      },
      "source": [
        "**1) eigen-decomposition을 이용한 PCA 구하기**\n",
        "\n",
        "\n",
        "eigen-decomposition을 이용해 PCA를 구하기 위해서는 먼저 공분산 행렬을 구해야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztnaNlYt0aU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_cen = X - X.mean(axis=0)  # 평균을 0으로\n",
        "X_cov = np.dot(X_cen.T, X_cen) / 59\n",
        "print(X_cov)\n",
        "\n",
        "# np.cov()를 이용해 구할 수도 있다.\n",
        "# print(np.cov(X_cen.T))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0opdPnL0win",
        "colab_type": "text"
      },
      "source": [
        "위에서 구한 공분산 행렬 X_cov에 대해 np.linalg.eig를 이용해 eigenvalue(w)와 eigenvector(v)를 구할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elMU8-b20aYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "w, v = np.linalg.eig(X_cov)\n",
        "\n",
        "print('eigenvalue :', w)\n",
        "print('eigenvector :\\n', v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh6yJYW_0ziH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('explained variance ratio :', w / w.sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLAk6D9w055U",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**2) SVD를 이용한 PCA 구하기**\n",
        "\n",
        "\n",
        "SVD는 공분산 행렬 X_cov을 사용하지 않고, X_cen의 singular value와 singular vector를 계산한다. 마찬가지로 np.linalg.svd를 이용하여 SVD를 구할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZt_ACIU05k-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "U, D, V_t = np.linalg.svd(X_cen)\n",
        "\n",
        "print('singular value :', D)\n",
        "print('singular vector :\\n', V_t.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVrGut340z5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print('explained variance ratio :', D ** 2 / np.sum(D**2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBIZcbCx1C1W",
        "colab_type": "text"
      },
      "source": [
        "**3) Scikit-Learn을 이용한 PCA 구하기**\n",
        "\n",
        "\n",
        "Scikit-Learn의 PCA를 이용해 간단하게 구할 수 있다. Scikit-Learn은 편차 또한 자동으로 처리해 계산해준다. 아래의 코드에서 singular vector 즉 주성분 행렬을 보면 위의 결과와 부호(-)가 다른것을 확인할 수 있다. 이는 벡터의 방향만 반대일 뿐 주성분 벡터가 구성하는 축은 동일하다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RORRnFHl0z_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "pca.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UECByjW91JiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('singular value :', pca.singular_values_)\n",
        "print('singular vector :\\n', pca.components_.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1sqNiy41Jqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# MNIST load\n",
        "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
        "\n",
        "# reshape\n",
        "train_x = train_x.reshape(-1, 28*28) \n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_reduced = pca.fit_transform(train_x)  # PCA 계산 후 투영"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grGE8T1p1c-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('선택한 차원(픽셀) 수 :', pca.n_components_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvPf4nyY1qW_",
        "colab_type": "text"
      },
      "source": [
        "PCA 적용결과 총 784 차원에서 154로 80%정도 차원이 축소된 것을 알 수 있다. 이러한 압축한 데이터셋을 이용해 SVM과 같은 분류알고리즘을 학습 시킬 경우 학습 속도를 빠르게 할 수 있다.\n",
        "\n",
        "또한, 압축된 데이터셋에 PCA 투영 변환을 반대로 적용하여 다시 원 데이터의 차원(mnist 경우 784)로 복원할 수 있다. 위에서 5% 만큼의 정보(분산)을 잃었기 때문에 완벽하게 복원은 할 수 없지만, 원본 데이터와 비슷하게 복원할 수 있다. 이러한 원본 데이터와 복원한 데이터간의 평균 제곱 거리를 재구성 오차(reconstruction error)라고 한다. 압축 후 복원하는 과정을 식으로 나타내면 다음과 같다\n",
        "\n",
        "아래의 예제는 위의 예제에서 압축한 X_reduced에다가 PCA의 inverse_transform()메소드를 이용해 784차원으로 복원한 코드이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNNZ6Isq1dHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "X_recovered = pca.inverse_transform(X_reduced)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhL_SVKM1dMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_digits(instances, images_per_row=5, **options):\n",
        "    size = 28\n",
        "    images_per_row = min(len(instances), images_per_row)\n",
        "    images = [instance.reshape(size,size) for instance in instances]\n",
        "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
        "    row_images = []\n",
        "    n_empty = n_rows * images_per_row - len(instances)\n",
        "    images.append(np.zeros((size, size * n_empty)))\n",
        "    for row in range(n_rows):\n",
        "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
        "        row_images.append(np.concatenate(rimages, axis=1))\n",
        "    image = np.concatenate(row_images, axis=0)\n",
        "    plt.imshow(image, cmap = matplotlib.cm.binary, **options)\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuJVWQ1K1dQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(7, 4))\n",
        "plt.subplot(121)\n",
        "plot_digits(train_x[::2100])\n",
        "plt.title(\"원본\", fontsize=16)\n",
        "plt.subplot(122)\n",
        "plot_digits(X_recovered[::2100])\n",
        "plt.title(\"압축 후 복원\", fontsize=16)\n",
        "plot_digits(X_recovered[::2100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol43rzJZTDpy",
        "colab_type": "text"
      },
      "source": [
        "# Kennel PCA\n",
        "---\n",
        "서포트벡터머신, SVM'에서 커널을 이용해 데이터를 저차원에서 고차원으로 매핑시켜 비선형 데이터셋에 SVM을 적용시키는 Kernel SVM에 대해 알아보았다.\n",
        "\n",
        "이렇게 같은 기법을 PCA에 적용해 **비선형 투영으로 차원을 축소**할 수 있는데, 이것을 Kernel PCA(KPCA)라고 한다. Scikit-Learn의 KernelPCA를 통해 Kernel PCA를 적용할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EoVRekA2ScR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.datasets import make_swiss_roll\n",
        "\n",
        "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc8QgwEO2Sfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\n",
        "X_reduced = rbf_pca.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz86MAPW2Sju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_pca = KernelPCA(n_components = 2, kernel=\"linear\", fit_inverse_transform=True)\n",
        "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\n",
        "sig_pca = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.001, coef0=1, fit_inverse_transform=True)\n",
        "\n",
        "y = t > 6.9\n",
        "\n",
        "plt.figure(figsize=(11, 4))\n",
        "for subplot, pca, title in ((131, lin_pca, \"선형 커널\"), (132, rbf_pca, \"RBF 커널, $\\gamma=0.04$\"), (133, sig_pca, \"시그모이드 커널, $\\gamma=10^{-3}, r=1$\")):\n",
        "    X_reduced = pca.fit_transform(X)\n",
        "    if subplot == 132:\n",
        "        X_reduced_rbf = X_reduced\n",
        "    \n",
        "    plt.subplot(subplot)\n",
        "    #plt.plot(X_reduced[y, 0], X_reduced[y, 1], \"gs\")\n",
        "    #plt.plot(X_reduced[~y, 0], X_reduced[~y, 1], \"y^\")\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)\n",
        "    plt.xlabel(\"$z_1$\", fontsize=18)\n",
        "    if subplot == 131:\n",
        "        plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk6oMC7z2kFR",
        "colab_type": "text"
      },
      "source": [
        "# 그 외 차원축소 알고리즘\n",
        "위에서 살펴본 PCA 차원 축소 알고리즘 외에 LLE , MDS, IsoMap, t-SNE, LDA 등 다양한 차원 축소 알고리즘들이 있다. 그 중에서 인기있는 차원 축소 알고리즘을 간단하게 알아보도록 하자.\n",
        "\n",
        "- MDS(Multi-Dimensional Scaling): MDS는 데이터 포인트 간의 거리를 보존하면서 차원을 축소하는 기법이다.\n",
        "- Isomap: Isomap은 각 데이터 포인트를 가장 가까운 이웃과 연결하는 식의 그래프를 만든 후 그래프에서 두 노드 사이의 최단 경로를 이루는 노드의 수인 geodesic distance를 유지 하면서 차원을 축소한다.\n",
        "- t-SNE(t-distributed Stochastic Neighbor Embedding): t-SNE는 비슷한 데이터는 가까이, 비슷하지 않은 데이터는 멀리 떨어지도록 차원을 축소한다. 주로 시각화에 많이 사용되며, 특히 고차원 공간에 있는 데이터의 군집을 시각화할 때 사용한다.\n",
        "- LDA: LDA는 Supervised learning이며, 분류 알고리즘에 속한다. LDA는 학습 단계에서 클래스를 가장 잘 구분하는 축을 학습하며, 이 축은 데이터가 투영되는 초평면을 정의하는 데 사용할 수 있다. 이러한 초평면으로 데이터를 투영하게 되면 클래스 간의 거리를 멀리 떨어지게 축소할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzT9bA2y3NKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.datasets import make_swiss_roll\n",
        "\n",
        "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=41)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oM-mVdgX3Nt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import MDS, Isomap, TSNE\n",
        "\n",
        "# MDS\n",
        "mds = MDS(n_components=2, random_state=42)\n",
        "X_reduced_mds = mds.fit_transform(X)\n",
        "\n",
        "# Isomap\n",
        "isomap = Isomap(n_components=2)\n",
        "X_reduced_isomap = isomap.fit_transform(X)\n",
        "\n",
        "# t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_reduced_tsne = tsne.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm9R1y-N3QAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "titles = [\"MDS\", \"Isomap\", \"t-SNE\"]\n",
        "\n",
        "plt.figure(figsize=(11,4))\n",
        "\n",
        "for subplot, title, X_reduced in zip((131, 132, 133), titles,\n",
        "                                     (X_reduced_mds, X_reduced_isomap, X_reduced_tsne)):\n",
        "    plt.subplot(subplot)\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)\n",
        "    plt.xlabel(\"$z_1$\", fontsize=18)\n",
        "    if subplot == 131:\n",
        "        plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfKwiISW55pO",
        "colab_type": "text"
      },
      "source": [
        "# 참고자료\n",
        "\n",
        "\n",
        "\n",
        "- StatQuest: Principal Component Analysis 주성분 분석 (PCA), 스텝 바이 스텝\n",
        "https://www.youtube.com/watch?v=FgakZw6K1QQ&feature=youtu.be\n",
        "\n",
        "- Principal Component Analysis\n",
        "(Explained Visually)\n",
        "https://setosa.io/ev/principal-component-analysis/\n",
        "\n",
        "- https://rpubs.com/Evan_Jung/pca\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56lGv1lp55VZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}