{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Decision Tree & Random Forest.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMbYNIlUtWMSs3R6eMZKwb4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghuilee91/Deeplearning/blob/master/Decision_Tree_%26_Random_Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GUIdKhsWZgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "import sys\n",
        "sys.path.append(\"/content/gdrive/My Drive/Colab Notebooks\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0GPpjDhWeHJ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Decision Tree (결정트리, 의사결정나무)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaZjGbd0WeTQ",
        "colab_type": "text"
      },
      "source": [
        "## 모델 소개\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "- 의사결정나무 학습법(Decision tree learning)은 어떤 항목에 대한 관측값과 목표값을 연결시켜주는 예측 모델로써 결정 트리를 사용한다. <br>\n",
        " *스무고개 놀이의  개념*\n",
        "\n",
        "- **분류와 회귀** 모두 가능한 모델이다. 즉 범주나 연속형 수치 모두 예측 가능하다. (분류나무, 회귀나무로 나뉜다)\n",
        "\n",
        "- 분류의 경우 최빈값, 회귀의 경우 종속변수(y)의 평균을 예측값으로 반환한다.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "![의사결정나무 예시](https://miro.medium.com/max/1380/1*xzF10JmR3K0rnZ8jtIHI_g.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enYaOTGFWebf",
        "colab_type": "text"
      },
      "source": [
        "## 불순도 / 불확실성\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "- 분류나무는 구분 뒤 각 영역의 순도(homogeneity)가 증가, 불순도(impurity) 혹은 불확실성(uncertainty)이 최대한 감소하도록 하는 방향으로 학습을 진행한다.\n",
        "\n",
        "- 순도가 증가/불확실성이 감소하는 걸 두고 정보이론에서는 **정보획득**(information gain)이라고 한다.\n",
        " <br> <br>\n",
        "#### 순도를 계산하는 3가지 방식\n",
        " 1. 엔트로피(Entropy)\n",
        "\n",
        " 2. 지니계수(Gini Index)\n",
        "\n",
        " 2. 오분류오차(Misclassification Error) <br>\n",
        "  '불순도를 측정할 수 있긴 하나 위 두 지표와 달리 미분이 불가능한 점 때문에 자주 쓰이지 않음 \n",
        "\n",
        "\n",
        "<br>\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1B_l1ovc_xl",
        "colab_type": "text"
      },
      "source": [
        "> **엔트로피**(entropy) <br>\n",
        "*얼마만큼의 정보를 담고 있는가*\n",
        "\n",
        "\n",
        "- 분포 p를 가진 H(X)로 표기되는 확률변수의 정보 \n",
        "-  **불확실성의 측정** 으로 기억!\n",
        "- 즉  확률 값임으로 0 ~ 1 사이의 실수를 가지며, 늘 양수임\n",
        "\n",
        "엔트로피가 '0'이라는 말은 현상이 발생할 확률이 0% 라는 것이다. 즉, 사건의 분포가 결정적(deterministic)이라면 해당 확률분포의 불확실성 정도를 나타내는 엔트로피는 낮아진다. 반대로 분포가 균등적(uniform)일 수록 엔트로피는 높아진다. (ex. 동전던지기, 엔트로피는 1이다. 결과값을 예상하기가 어렵기 때문이다.(불확실성 최대치)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ut3gv_ygwP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import List\n",
        "import math\n",
        "\n",
        "def entropy(class_probabilities: List[float]) -> float:\n",
        "    \"\"\"Given a list of class probabilities, compute the entropy\"\"\"\n",
        "    return sum(-p * math.log(p, 2)\n",
        "               for p in class_probabilities\n",
        "               if p > 0)                     # ignore zero probabilities\n",
        "\n",
        "assert entropy([1.0]) == 0\n",
        "assert entropy([0.5, 0.5]) == 1\n",
        "assert 0.81 < entropy([0.25, 0.75]) < 0.82\n",
        "\n",
        "from typing import Any\n",
        "from collections import Counter\n",
        "\n",
        "def class_probabilities(labels: List[Any]) -> List[float]:\n",
        "    total_count = len(labels)\n",
        "    return [count / total_count\n",
        "            for count in Counter(labels).values()]\n",
        "\n",
        "def data_entropy(labels: List[Any]) -> float:\n",
        "    return entropy(class_probabilities(labels))\n",
        "\n",
        "assert data_entropy(['a']) == 0\n",
        "assert data_entropy([True, False]) == 1\n",
        "assert data_entropy([3, 4, 4, 4]) == entropy([0.25, 0.75])\n",
        "\n",
        "def partition_entropy(subsets: List[List[Any]]) -> float:\n",
        "    \"\"\"Returns the entropy from this partition of data into subsets\"\"\"\n",
        "    total_count = sum(len(subset) for subset in subsets)\n",
        "\n",
        "    return sum(data_entropy(subset) * len(subset) / total_count\n",
        "               for subset in subsets)\n",
        "\n",
        "from typing import NamedTuple, Optional\n",
        "\n",
        "class Candidate(NamedTuple):\n",
        "    level: str\n",
        "    lang: str\n",
        "    tweets: bool\n",
        "    phd: bool\n",
        "    did_well: Optional[bool] = None  # allow unlabeled data\n",
        "\n",
        "                  #  level     lang     tweets  phd  did_well\n",
        "inputs = [Candidate('Senior', 'Java',   False, False, False),\n",
        "          Candidate('Senior', 'Java',   False, True,  False),\n",
        "          Candidate('Mid',    'Python', False, False, True),\n",
        "          Candidate('Junior', 'Python', False, False, True),\n",
        "          Candidate('Junior', 'R',      True,  False, True),\n",
        "          Candidate('Junior', 'R',      True,  True,  False),\n",
        "          Candidate('Mid',    'R',      True,  True,  True),\n",
        "          Candidate('Senior', 'Python', False, False, False),\n",
        "          Candidate('Senior', 'R',      True,  False, True),\n",
        "          Candidate('Junior', 'Python', True,  False, True),\n",
        "          Candidate('Senior', 'Python', True,  True,  True),\n",
        "          Candidate('Mid',    'Python', False, True,  True),\n",
        "          Candidate('Mid',    'Java',   True,  False, True),\n",
        "          Candidate('Junior', 'Python', False, True,  False)\n",
        "         ]\n",
        "\n",
        "from typing import Dict, TypeVar\n",
        "from collections import defaultdict\n",
        "\n",
        "T = TypeVar('T')  # generic type for inputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0s9HXyHh2p7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1 단계 : 가장 낮은 엔트로피를 반환하는 파티션 찾기\n",
        "\n",
        "def partition_by(inputs: List[T], attribute: str) -> Dict[Any, List[T]]:\n",
        "    \"\"\"Partition the inputs into lists based on the specified attribute.\"\"\"\n",
        "    partitions: Dict[Any, List[T]] = defaultdict(list)\n",
        "    for input in inputs:\n",
        "        key = getattr(input, attribute)  # value of the specified attribute\n",
        "        partitions[key].append(input)    # add input to the correct partition\n",
        "    return partitions\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1eZXoeMh-6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2 단계 : 엔트로피를 계산\n",
        "def partition_entropy_by(inputs: List[Any],\n",
        "                         attribute: str,\n",
        "                         label_attribute: str) -> float:\n",
        "    \"\"\"Compute the entropy corresponding to the given partition\"\"\"\n",
        "    # partitions consist of our inputs\n",
        "    partitions = partition_by(inputs, attribute)\n",
        "\n",
        "    # but partition_entropy needs just the class labels\n",
        "    labels = [[getattr(input, label_attribute) for input in partition]\n",
        "              for partition in partitions.values()]\n",
        "\n",
        "    return partition_entropy(labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjcQoPTqiNSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# 3 단계 : 전체 데이터셋에 대해 엔트로피를 최소화하는 판티션 찾기\n",
        "for key in ['level','lang','tweets','phd']:\n",
        "    print(key, partition_entropy_by(inputs, key, 'did_well'))\n",
        "\n",
        "# 직급에 대해 파티션을 나눌 때 엔트로피가 최솟값을 가진다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bwhXXS1idxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter, defaultdict\n",
        "from functools import partial\n",
        "import math, random\n",
        "\n",
        "def entropy(class_probabilities):\n",
        "    \"\"\"given a list of class probabilities, compute the entropy\"\"\"\n",
        "    return sum(-p * math.log(p, 2) for p in class_probabilities if p)\n",
        "\n",
        "def class_probabilities(labels):\n",
        "    total_count = len(labels)\n",
        "    return [count / total_count\n",
        "            for count in Counter(labels).values()]\n",
        "\n",
        "def data_entropy(labeled_data):\n",
        "    labels = [label for _, label in labeled_data]\n",
        "    probabilities = class_probabilities(labels)\n",
        "    return entropy(probabilities)\n",
        "\n",
        "def partition_entropy(subsets):\n",
        "    \"\"\"find the entropy from this partition of data into subsets\"\"\"\n",
        "    total_count = sum(len(subset) for subset in subsets)\n",
        "\n",
        "    return sum( data_entropy(subset) * len(subset) / total_count\n",
        "                for subset in subsets )\n",
        "\n",
        "def group_by(items, key_fn):\n",
        "    \"\"\"returns a defaultdict(list), where each input item\n",
        "    is in the list whose key is key_fn(item)\"\"\"\n",
        "    groups = defaultdict(list)\n",
        "    for item in items:\n",
        "        key = key_fn(item)\n",
        "        groups[key].append(item)\n",
        "    return groups\n",
        "\n",
        "def partition_by(inputs, attribute):\n",
        "    \"\"\"returns a dict of inputs partitioned by the attribute\n",
        "    each input is a pair (attribute_dict, label)\"\"\"\n",
        "    return group_by(inputs, lambda x: x[0][attribute])\n",
        "\n",
        "def partition_entropy_by(inputs,attribute):\n",
        "    \"\"\"computes the entropy corresponding to the given partition\"\"\"\n",
        "    partitions = partition_by(inputs, attribute)\n",
        "    return partition_entropy(partitions.values())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLDxX-3LjkZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def classify(tree, input):\n",
        "    \"\"\"classify the input using the given decision tree\"\"\"\n",
        "\n",
        "    # if this is a leaf node, return its value\n",
        "    if tree in [True, False]:\n",
        "        return tree\n",
        "\n",
        "    # otherwise find the correct subtree\n",
        "    attribute, subtree_dict = tree\n",
        "\n",
        "    subtree_key = input.get(attribute)  # None if input is missing attribute\n",
        "\n",
        "    if subtree_key not in subtree_dict: # if no subtree for key,\n",
        "        subtree_key = None              # we'll use the None subtree\n",
        "\n",
        "    subtree = subtree_dict[subtree_key] # choose the appropriate subtree\n",
        "    return classify(subtree, input)     # and use it to classify the input\n",
        "\n",
        "def build_tree_id3(inputs, split_candidates=None):\n",
        "\n",
        "    # if this is our first pass,\n",
        "    # all keys of the first input are split candidates\n",
        "    if split_candidates is None:\n",
        "        split_candidates = inputs[0][0].keys()\n",
        "\n",
        "    # count Trues and Falses in the inputs\n",
        "    num_inputs = len(inputs)\n",
        "    num_trues = len([label for item, label in inputs if label])\n",
        "    num_falses = num_inputs - num_trues\n",
        "\n",
        "    if num_trues == 0:                  # if only Falses are left\n",
        "        return False                    # return a \"False\" leaf\n",
        "\n",
        "    if num_falses == 0:                 # if only Trues are left\n",
        "        return True                     # return a \"True\" leaf\n",
        "\n",
        "    if not split_candidates:            # if no split candidates left\n",
        "        return num_trues >= num_falses  # return the majority leaf\n",
        "\n",
        "    # otherwise, split on the best attribute\n",
        "    best_attribute = min(split_candidates,\n",
        "        key=partial(partition_entropy_by, inputs))\n",
        "\n",
        "    partitions = partition_by(inputs, best_attribute)\n",
        "    new_candidates = [a for a in split_candidates\n",
        "                      if a != best_attribute]\n",
        "\n",
        "    # recursively build the subtrees\n",
        "    subtrees = { attribute : build_tree_id3(subset, new_candidates)\n",
        "                 for attribute, subset in partitions.items() }\n",
        "\n",
        "    subtrees[None] = num_trues > num_falses # default case\n",
        "\n",
        "    return (best_attribute, subtrees)\n",
        "\n",
        "def forest_classify(trees, input):\n",
        "    votes = [classify(tree, input) for tree in trees]\n",
        "    vote_counts = Counter(votes)\n",
        "    return vote_counts.most_common(1)[0][0]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    inputs = [\n",
        "        ({'level':'Senior','lang':'Java','tweets':'no','phd':'no'},   False),\n",
        "        ({'level':'Senior','lang':'Java','tweets':'no','phd':'yes'},  False),\n",
        "        ({'level':'Mid','lang':'Python','tweets':'no','phd':'no'},     True),\n",
        "        ({'level':'Junior','lang':'Python','tweets':'no','phd':'no'},  True),\n",
        "        ({'level':'Junior','lang':'R','tweets':'yes','phd':'no'},      True),\n",
        "        ({'level':'Junior','lang':'R','tweets':'yes','phd':'yes'},    False),\n",
        "        ({'level':'Mid','lang':'R','tweets':'yes','phd':'yes'},        True),\n",
        "        ({'level':'Senior','lang':'Python','tweets':'no','phd':'no'}, False),\n",
        "        ({'level':'Senior','lang':'R','tweets':'yes','phd':'no'},      True),\n",
        "        ({'level':'Junior','lang':'Python','tweets':'yes','phd':'no'}, True),\n",
        "        ({'level':'Senior','lang':'Python','tweets':'yes','phd':'yes'},True),\n",
        "        ({'level':'Mid','lang':'Python','tweets':'no','phd':'yes'},    True),\n",
        "        ({'level':'Mid','lang':'Java','tweets':'yes','phd':'no'},      True),\n",
        "        ({'level':'Junior','lang':'Python','tweets':'no','phd':'yes'},False)\n",
        "    ]\n",
        "\n",
        "    for key in ['level','lang','tweets','phd']:\n",
        "        print(key, partition_entropy_by(inputs, key))\n",
        "    print()\n",
        "\n",
        "    senior_inputs = [(input, label)\n",
        "                     for input, label in inputs if input[\"level\"] == \"Senior\"]\n",
        "\n",
        "    for key in ['lang', 'tweets', 'phd']:\n",
        "        print(key, partition_entropy_by(senior_inputs, key))\n",
        "    print()\n",
        "\n",
        "    print(\"building the tree\")\n",
        "    tree = build_tree_id3(inputs)\n",
        "    print(tree)\n",
        "\n",
        "    print(\"Junior / Java / tweets / no phd\", classify(tree,\n",
        "        { \"level\" : \"Junior\",\n",
        "          \"lang\" : \"Java\",\n",
        "          \"tweets\" : \"yes\",\n",
        "          \"phd\" : \"no\"} ))\n",
        "\n",
        "    print(\"Junior / Java / tweets / phd\", classify(tree,\n",
        "        { \"level\" : \"Junior\",\n",
        "                 \"lang\" : \"Java\",\n",
        "                 \"tweets\" : \"yes\",\n",
        "                 \"phd\" : \"yes\"} ))\n",
        "\n",
        "    print(\"Intern\", classify(tree, { \"level\" : \"Intern\" } ))\n",
        "    print(\"Senior\", classify(tree, { \"level\" : \"Senior\" } ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXNUJCERVMdU",
        "colab_type": "text"
      },
      "source": [
        "### - 분류나무 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG0uKqAsVFMN",
        "colab_type": "text"
      },
      "source": [
        "### - 회귀나무 모델\n",
        "#### 모델링 프로세스 \n",
        "- 데이터를 M개로 분할 R1,R2 ...Rm\n",
        "- 최상의 분할은 다음 비용함수를 최소로 할 때 얻어짐\n",
        "- 각 분할에 속해 있는 y값들의 평균으로 예측했을 때 오류가 최소됨\n",
        "- 분할변수(j)와 분할점(s)은 어떻게 결정할까?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2ENjKb5Wei7",
        "colab_type": "text"
      },
      "source": [
        "## 모델 학습\n",
        "---\n",
        "<br>\n",
        "\n",
        "의사결정나무의 학습 과정은 입력 변수 영역을 두 개로 구분하는 **재귀적 분기(recursive partitioning)** 와 너무 자세하게 구분된 영역을 통합하는 **가지치기(pruning)** 두 가지로 과정으로 나뉜다.\n",
        "\n",
        "### 1) 재귀적 분기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irGfDdYmdxex",
        "colab_type": "text"
      },
      "source": [
        "### 2) 가지치기 (Pruning)\n",
        "- Terminal node의 순도가 100%인 상태를 Full tree라고 한다. 분기가 너무 많을 경우 학습데이터에 과적합(overfitting)할 우려가 있기 때문에 적절한 수준에서 **가지치기** 가 필요하다.\n",
        "\n",
        "- 결정나무의 분기 수가 일정한 수준 이상이 되면 오분류율이 오히려 증가하는 현상이 발생한다. 이에 검증데이터에 대한 오분류율이 증가하는 시점을 파악하여 적절히 가지치기를 수행해야 한다.\n",
        "\n",
        "- 나뭇가지를 잘라내는 것과 같다는 의미에서 용어가 붙었으나 실제 개념은 분기를 합치는 Merge이 개념으로 이해해야 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NidAQ7VwWel1",
        "colab_type": "text"
      },
      "source": [
        "## 장단점 및 활용\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "#### 장점\n",
        "- 숫자형 데이터와 범주형 데이터를 동시에 다룰 수 있음\n",
        "- 특정 변수의 값이 누락되어도 사용 가능\n",
        "- 계산복잡성 대비 높은 예측 성능\n",
        "- 변수 단위 설명력(이해하고 해석하귀 용이함)\n",
        "\n",
        "\n",
        "#### 단점\n",
        "- 새로운 데이터에 대한 일반화 성능이 좋지 않아 오버피팅이 되기 쉬움( = 결정경계(decision boundary)가 데이터 축에 \n",
        "수직이어서 특정 데이터에만 잘 작동할 가능성 크다!)\n",
        "<br>\n",
        "\n",
        " ☞ *이를 극복하기 위해 **랜덤포레스트** 모델이 등장함*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmYtxzoJWepO",
        "colab_type": "text"
      },
      "source": [
        "[참고-코드]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bROMSc9EeAtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def entropy(class_probabilities):\n",
        "    \"\"\"클래스에 속할 확률을 입력하면 엔트로피를 계산하라\"\"\"\n",
        "    return sum(-p * math.log(p,2)\n",
        "    for p in class_probabilities if p)\n",
        "\n",
        "def class_probabilities(labels):\n",
        "    total_count = len(labels)\n",
        "    return [count / total_count for count in Counter(labels).values()]\n",
        "\n",
        "def data_entropy(labeled_data):\n",
        "    labels = [label for _, label in labeled_data]\n",
        "    probabilities = class_probabilities(labels)\n",
        "    return entropy(probabilities)\n",
        "\n",
        "def partition_entropy(subsets):\n",
        "    total_count = sum(len(subset) for subset in subsets)\n",
        "    return sum( data_entropy(subset) * len(subset / total_count\n",
        "                                           for subset in subsets))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnV_MQiOWexO",
        "colab_type": "text"
      },
      "source": [
        "# Random Forest(랜덤 포레스트)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_BOVDZGWeuo",
        "colab_type": "text"
      },
      "source": [
        "- 의사결정나무가 학습 데이터에 꼭 들어맞는다? = 오버피팅\n",
        "- 오버피팅을 방지할 수 있는 대표적인 방법 중 하나로 랜덤포레스트라는 것이 등장함\n",
        "- 여러 개의 의사결정나무를 만들고, 그들의 다수결로 결과를 결정하는 방법\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do5jVKOlkV7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forest_classify(trees, input):\n",
        "    votes = [classify(tree, input) for tree in trees]\n",
        "    vote_counts = Counter(votes)\n",
        "    return vote_counts.most_common(1)[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK7l7mxVWesp",
        "colab_type": "text"
      },
      "source": [
        "## 부트스트랩(Bootstrap)\n",
        "- 전체 데이터가 inputs라고 했을 때 그것을 전부 이용해서 나무를 학습하는 것이 아니라 bootstrap_sample(inputs)의 결과물을 각 나무의 입력값으로 넣어 학습하는 것\n",
        "\n",
        "- 각 나무가 서로 다른 데이터로 구축되기 때문에 랜덤성이 생기게 된다. (이점 : 샘플링되지 않는 데이터는 테스트 데이터로 이용 가능, 즉 성능을 측정하는 방법만 잘 설계한다면, 데이터 전체를 학습에 사용해도 된다는 것을 의미한다.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6blYilNgWeWT",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}