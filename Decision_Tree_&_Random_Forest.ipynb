{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Decision Tree & Random Forest.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNg3ZcG0zpQcsh3DXEHPd0o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghuilee91/Deeplearning/blob/master/Decision_Tree_%26_Random_Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GUIdKhsWZgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "import sys\n",
        "sys.path.append(\"/content/gdrive/My Drive/Colab Notebooks\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0GPpjDhWeHJ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Decision Tree (결정트리, 의사결정나무)\n",
        "\n",
        "---\n",
        "분류함수를 의사결정 규칙으로 표현할 때 타원(분기점), 직선(가지), 사각형(잎사귀)을 이용하여 나무형태로 그려서 분석하는 기법\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaZjGbd0WeTQ",
        "colab_type": "text"
      },
      "source": [
        "## 모델 소개\n",
        "<br>\n",
        "\n",
        "- 의사결정나무 학습법(Decision tree learning)은 어떤 항목에 대한 관측값과 목표값을 연결시켜주는 예측 모델로써 결정 트리를 사용한다. <br>\n",
        " *스무고개 놀이의  개념*\n",
        "\n",
        "- **분류와 회귀** 모두 가능한 모델이다. 즉 범주나 연속형 수치 모두 예측 가능하다. (분류나무, 회귀나무로 나뉜다)\n",
        "\n",
        "- 분류의 경우 최빈값, 회귀의 경우 종속변수(y)의 평균을 예측값으로 반환한다.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "![의사결정나무 예시](https://miro.medium.com/max/1380/1*xzF10JmR3K0rnZ8jtIHI_g.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXNUJCERVMdU",
        "colab_type": "text"
      },
      "source": [
        "### - 분류나무 모델(범주)\n",
        "- 각 관측치 마다 반응변수 값 yi=1,2,...K,. 즉 K개의 클래스(범주) 존재\n",
        "- Rm : 끝노드 m에 해당하며 Nm 관측치 개수를 가지고 있음\n",
        "- P^mk : 끝노드m에서 k클래스에서 속해 있는 관측치의 비율 \n",
        "- 끝노드 m으로 분류된 관측치는 **k(m)클래스로 분류**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG0uKqAsVFMN",
        "colab_type": "text"
      },
      "source": [
        "### - 회귀나무 모델(예측)\n",
        "- Cm : 회귀나무모델로 부터 예측한 Rm 부분의 예측값\n",
        "- 데이터를 M개로 분할 R1,R2,....Rm \n",
        "- 최상의 분할은 다음 비용함수(cost function)를 최소로 할 때 얻어짐\n",
        "- 각 분할에 속해 있는 **y값들의 평균**으로 예측했을 때 **오류가 최소**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06_QCtBDQBc0",
        "colab_type": "text"
      },
      "source": [
        "<br> ![대체 텍스트](https://i.imgur.com/ZKDnzOB.png)\n",
        "\n",
        "\n",
        "###[분류]\n",
        ": 새로운 데이터가 특정 terminal node에 속한다는 정보를 확인한 뒤 해당 terminal node에서 가장 빈도가 높은 범주에 새로운 데이터를 분류 <br>\n",
        "\n",
        "- 날씨는 맑은데 습도가 70을 넘는 날은 경기가 열리지 않을 거라고 예측\n",
        "\n",
        "###[회귀]\n",
        ":  terminal node의 종속변수(y)의 평균을 예측값으로 반환, 이 때 예측값의 종류는 terminal node 개수와 일치함. <br>\n",
        "\n",
        "- 예로 Terminal node 수가 3개뿐이라면 새로운 데이터가 100개, 아니 1000개가 주어진다고 해도 의사결정나무는 딱 3종류의 답만을 출력\n",
        "\n",
        "<br>\n",
        "\n",
        "새로운 데이터가 주어졌을 때 의사결정나무는 해당 데이터의 집합을 대표할 수 있는 값(분류=최빈값, 회귀=평균)을 반환하는 방식으로 예측"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Flx9txF-HZ3",
        "colab_type": "text"
      },
      "source": [
        "## 분할법칙\n",
        "\n",
        "*그렇다면 대체 어떤 기준으로 영역을 나누는 걸까?*\n",
        "\n",
        "---\n",
        "\n",
        "- 분할변수와 분할기준(점)은 목표변수(y)의 분포를 가장 잘 구별해주는 쪽으로 정함\n",
        "\n",
        "- 목표변수의 분포를 잘 구별해주는 측도로 순수도(purity) 또는 불순도(impurity)를 정의\n",
        "\n",
        "- 예를 들어 클래스 0과 클래스 1의 비율이 45%, 55% 인 노드는 각 클래스의 비율이 90%와 10%인 마디에 비하여 순수도가 낮다(또는 불순도가 높다)라고 해석\n",
        "\n",
        "- 각 노드에서 분할변수와 분할점의 설정은 불순도의 감소가 최대가 되도록 선택\n",
        "<br>\n",
        "\n",
        "#### [알고리즘 프로세스]\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbGlghI%2FbtqwYFXZzCu%2F0g4cMFuumUkKDYmDfkMdu0%2Fimg.png\">\n",
        "\n",
        "먼저 위와 같이 데이터를 가장 잘 구분할 수 있는 질문을 기준으로 나눔\n",
        "\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbJSlvg%2FbtqwXHvdrPJ%2FZhikSUKx3SmuYZSz6NGZL1%2Fimg.png\">\n",
        "\n",
        "나뉜 각 범주에서 또 다시 데이터를 가장 잘 구분할 수 있는 질문을 기준으로 나눔 이를 지나치게 많이 하면 아래와 같이 오버피팅이 됩니다. 결정 트리에 아무 파라미터를 주지 않고 모델링하면 오버피팅 야기됨\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbUvjhU%2FbtqwYiIK85s%2FoQ3KuTZVk6CgSAQI0VkwW1%2Fimg.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enYaOTGFWebf",
        "colab_type": "text"
      },
      "source": [
        "## 비용함수\n",
        "---\n",
        "분류나무는 구분 뒤 각 영역의 순도(homogeneity)가 증가, 불순도(impurity) 혹은 불확실성(uncertainty)이 최대한 감소하도록 하는 방향으로 학습을 진행한다.\n",
        "<br>\n",
        "<br>\n",
        "#### **불순도를 계산하는 3가지 비용 함수**\n",
        " 1. 엔트로피(Entropy)\n",
        "\n",
        " 2. 지니계수(Gini Index)\n",
        "\n",
        " 3. 오분류오차(Misclassification Error) <br>\n",
        "  '불순도를 측정할 수 있긴 하나 위 두 지표와 달리 미분이 불가능한 점 때문에 자주 쓰이지 않음 \n",
        "\n",
        "\n",
        "<IMG SRC=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile4.uf.tistory.com%2Fimage%2F995CA63E5B1A4E6F389B19\">\n",
        "\n",
        "위의 그림을 보면 알수있듯이 Entropy 및 Gini와 Misclassification error은 0에 가까울수록 최적의 의사결정 트리를 만들 수 있게 됨\n",
        "(Y축 = 불순도)\n",
        "\n",
        "<br>\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1B_l1ovc_xl",
        "colab_type": "text"
      },
      "source": [
        "###  **엔트로피** <br>\n",
        "\n",
        "\n",
        "- 엔트로피(Entropy)는 불순도(Impurity)를 수치적으로 나타낸 척도\n",
        "- 즉  확률 값임으로 0 ~ 1 사이의 실수를 가지며, 늘 양수임\n",
        "- 트로피가 높다는 것은 불순도도 높다는 뜻이고, 엔트로피가 낮다는 것은 불순도도 낮다는 뜻\n",
        "- 즉, 사건의 분포가 결정적(deterministic)이라면 해당 확률분포의 불확실성 정도를 나타내는 엔트로피는 낮아짐\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FpL6pO%2FbtqwVDN1V94%2FTYgn5iFrPTfgdVwZhxVKl1%2Fimg.png\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0s9HXyHh2p7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### 분할변수와 분할점은 어떻게 결정할까?\n",
        "# 1 단계 : 가장 낮은 엔트로피를 반환하는 파티션 찾기\n",
        "\n",
        "def partition_by(inputs: List[T], attribute: str) -> Dict[Any, List[T]]:\n",
        "    \"\"\"Partition the inputs into lists based on the specified attribute.\"\"\"\n",
        "    partitions: Dict[Any, List[T]] = defaultdict(list)\n",
        "    for input in inputs:\n",
        "        key = getattr(input, attribute)  # value of the specified attribute\n",
        "        partitions[key].append(input)    # add input to the correct partition\n",
        "    return partitions\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1eZXoeMh-6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2 단계 : 엔트로피를 계산\n",
        "def partition_entropy_by(inputs: List[Any],\n",
        "                         attribute: str,\n",
        "                         label_attribute: str) -> float:\n",
        "    \"\"\"Compute the entropy corresponding to the given partition\"\"\"\n",
        "    # partitions consist of our inputs\n",
        "    partitions = partition_by(inputs, attribute)\n",
        "\n",
        "    # but partition_entropy needs just the class labels\n",
        "    labels = [[getattr(input, label_attribute) for input in partition]\n",
        "              for partition in partitions.values()]\n",
        "\n",
        "    return partition_entropy(labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjcQoPTqiNSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# 3 단계 : 전체 데이터셋에 대해 엔트로피를 최소화하는 판티션 찾기\n",
        "for key in ['level','lang','tweets','phd']:\n",
        "    print(key, partition_entropy_by(inputs, key, 'did_well'))\n",
        "\n",
        "# 직급에 대해 파티션을 나눌 때 엔트로피가 최솟값을 가진다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2ENjKb5Wei7",
        "colab_type": "text"
      },
      "source": [
        "## 모델 학습\n",
        "---\n",
        "<br>\n",
        "\n",
        "의사결정나무의 학습 과정은 입력 변수 영역을 두 개로 구분하는 **재귀적 분기(recursive partitioning)** 와 너무 자세하게 구분된 영역을 통합하는 **가지치기(pruning)** 두 가지로 과정으로 나뉜다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM5LMVzbNWWN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### 1) 재귀적 분기\n",
        "\n",
        "*= 순환, 반복하는 분리*\n",
        "\n",
        "의사결정 트리에서는 정보 획득량(Information Gain)이 큰 순서대로 질문을 배치하는 게 중요하다. (상식적인 거다. 스무고개를 하더라도 크리티컬한 질문을 먼저 하는 게 좋지 않은가.)\n",
        "\n",
        "그리고 그 속성에 대해 어느 기준으로 나누는 게 좋을지 반복적으로 적용해보면서 최적의 트리를 찾게 된다. 이를 재귀(recursive) 알고리즘이라 한다.\n",
        "\n",
        "즉 이렇게 반복하면서 찾으면서 더 이상 정보 획득량이 없을 때 재귀(recursion)를 중단하게 된다. 더 이상은 순수한 하위 집합을 만드는 방법을 찾을 수 없을 때까지 분류하는 과정을 재귀적 분기라고 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irGfDdYmdxex",
        "colab_type": "text"
      },
      "source": [
        "### 2) 가지치기\n",
        "- Terminal node의 순도가 100%인 상태를 Full tree라고 한다. 분기가 너무 많을 경우 학습데이터에 과적합(overfitting)할 우려가 있기 때문에 적절한 수준에서 **가지치기** 가 필요하다.\n",
        "\n",
        "- 결정나무의 분기 수가 일정한 수준 이상이 되면 오분류율이 오히려 증가하는 현상이 발생한다. 이에 검증데이터에 대한 오분류율이 증가하는 시점을 파악하여 적절히 가지치기를 수행해야 한다.\n",
        "\n",
        "- 나뭇가지를 잘라내는 것과 같다는 의미에서 용어가 붙었으나 실제 개념은 분기를 합치는 Merge이 개념으로 이해해야 한다.\n",
        "\n",
        "\n",
        "\n",
        "![대체 텍스트](https://i.imgur.com/5zhZIAw.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOFaaZZENyEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n",
        "tree = DecisionTreeClassifier(random_state=0)\n",
        "tree.fit(X_train, y_train)\n",
        "print(\"훈련 세트 정확도: {:.3f}\".format(tree.score(X_train, y_train)))\n",
        "print(\"테스트 세트 정확도: {:.3f}\".format(tree.score(X_test, y_test)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuRIquIoN2WU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# max_depth=4로 설정해주면 오버피팅을 막아 훈련 세트 정확도는 아까보다 떨어지지만 테스트 세트 정확도가 더 높아짐\n",
        "# pruning\n",
        "\n",
        "tree = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "print(\"훈련 세트 정확도: {:.3f}\".format(tree.score(X_train, y_train)))\n",
        "print(\"테스트 세트 정확도: {:.3f}\".format(tree.score(X_test, y_test)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NidAQ7VwWel1",
        "colab_type": "text"
      },
      "source": [
        "## 장점 및 단점\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "#### 장점\n",
        "- 숫자형 데이터와 범주형 데이터를 동시에 다룰 수 있음\n",
        "- 특정 변수의 값이 누락되어도 사용 가능\n",
        "- 계산복잡성 대비 높은 예측 성능\n",
        "- 변수 단위 설명력(이해하고 해석하귀 용이함)\n",
        "\n",
        "<br>\n",
        "\n",
        "#### 단점\n",
        "- 계층적 구조로 인해 중간에 에러가 발생하면 다음 단계로 에러가 계속 전파\n",
        "- 학습 데이터의 미세한 변동에도 최종 결과 크게 영향\n",
        "- 적은 개수의 노이즈에도 크게 영향\n",
        "- 나무의 최종노드 개수를 늘리면 과적합 위험(Low Bias, Large Variance)\n",
        "\n",
        "<br>\n",
        "\n",
        "> *이를 극복하기 위해 **랜덤포레스트** 모델이 등장함*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnV_MQiOWexO",
        "colab_type": "text"
      },
      "source": [
        "# Random Forest(랜덤 포레스트)\n",
        "---\n",
        "여러 개의 결정 트리들을 임의적으로 학습하는 방식의 앙상블 방법으로서, 여러가지 학습기들을 생성한 후 이를 선형 결합하여 최종 학습기를 만드는 방법\n",
        "\n",
        "\n",
        "<IMG SRC=\"http://itwiki.kr/images/thumb/1/12/%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8.png/750px-%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve0FqV2ICWF3",
        "colab_type": "text"
      },
      "source": [
        "## 랜덤 포레스트 배경 - 앙상블\n",
        "\n",
        " - 여러Base모델들의 예측을 다수결 법칙 또는 평균을 이용해 통합하여 예측 정확성을 향상시키는 방법\n",
        "<BR>\n",
        " - 다음 조건을 만족할 때 앙상블 모델은 Base 모델보다 우수한 성능을 보여줌\n",
        "\n",
        "<BR>\n",
        "\n",
        ">  Base 모델들이 서로 독립적<BR>\n",
        " Base 모델들이 무작위 예측을 수행하는 모델보다 성능이 좋은 경우\n",
        "   \n",
        "\n",
        "<BR>\n",
        "\n",
        "- **의사결정나무모델**은 앙상블 모델의 base모델로써 활용도가 높음\n",
        "<BR>\n",
        "\n",
        "> Low computational complexity : 데이터의 크기가 방대한 경우에도 모델을 빨리 구축할 수 <BR>\n",
        " Nonparamatric : 데이터 분포에 대한 전제가 필요하지 않음 (정규분포X, 지수분포X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_BOVDZGWeuo",
        "colab_type": "text"
      },
      "source": [
        "## 랜덤 포레스트 개요\n",
        "- 다수의 의사결정나무모델에 의한 예측을 종합하는 앙상블 방법<br>\n",
        "- 일반적으로 하나의 의사결정나무모델 보다 높은 예측 정확성을 보여줌<br>\n",
        "- 관측치 수에 비해 변수의 수가 많은 고차원 데이터에서 중요 변수 선택 기법으로 널리 활용됨<br><br>\n",
        "\n",
        "- 핵심아이디어 : **Diversity** ,**Random** 확보<br>\n",
        "   1) 여러 개의 Training data를 생성하여 데이터마다 개별 의사결정나무모델 구축 (Bagging)<br>\n",
        "   2) 의사결정나무모델 구축 시 변수 무작위로 선택 (Random subspace)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE5HJA7xEy1B",
        "colab_type": "text"
      },
      "source": [
        "## 1) Bagging (Boostrap Aggregating)\n",
        "부트스트랩(bootstrap)을 통해 조금씩 다른 훈련 데이터에 대해 훈련된 기초 분류기(base learner)들을 결합(aggregating)시키는 방법 <br>\n",
        "\n",
        "<IMG SRC=\"http://itwiki.kr/images/5/55/%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8_%EC%8B%9C%EA%B0%81%ED%99%94.png\">\n",
        "\n",
        "> ### **Boostrapping (=sampling)**\n",
        " - 각 모델은 서로 다른 학습 데이터셋 이용\n",
        " - 각 데이터셋은 복원 추출(sampling with replacement)을 이용해 원래 데이터의 수만큼 크기를 갖도록 샘플링\n",
        " - 개별 데이터셋을 부스트랩셋이라 부름\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTwhfi3oFyXt",
        "colab_type": "text"
      },
      "source": [
        "## 2) Random Subspace\n",
        "의사결정나무의 분기점을 탐색할 때, 원래 변수의 수보다 적은 수의 변수를 임의로 선택하여 해당 변수들만을 고려 대상으로 함\n",
        "\n",
        "\n",
        "<IMG SRC=\"http://itwiki.kr/images/5/55/%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8_%EA%B3%BC%EC%A0%95.gif\">\n",
        "\n",
        " 1. 원래 변수들 중에서 모델 구축에 쓰일 **입력 변수를 무작위로 선택**\n",
        " 2. 선택된 입력 변수 중에 분할될 변수를 선택\n",
        " 3. 이러한 과정을 full-grown tree가 될 때까지 반복함\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6blYilNgWeWT",
        "colab_type": "text"
      },
      "source": [
        "## 하이퍼 파라미터\n",
        "\n",
        "1. Decision tree의 수 \n",
        " - strong law of large numbers 을 만족시키기 위해 최소 2,000개 이상의 decision tree필요(case by case)\n",
        "\n",
        "2. Decision tree에서 노드 분할 시 무작위로 선택되는 변수의 수\n",
        " - 일반적으로 변수의 수에 따라 다음과 같이 추천됨 (Diaz-Uriarte at el., 2006)\n",
        " - Classification : 변수의 수의 제곱근 (루트) ex. 25개 변수라면, 5개\n",
        " - Regression : 변수의 수 / 3\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBFSSRl2Tt7u",
        "colab_type": "text"
      },
      "source": [
        "## 장점 및 단점\n",
        "---\n",
        "\n",
        "#### 장점\n",
        "- 의사결정나무 보다 정확도가 높음\n",
        "<br>\n",
        "\n",
        "#### 단점\n",
        "- 의사결정나무가 가지는 설명력을 상실함(변수의 가중치에 대한 우선순위 파악이 어려워 결과를 해석하기 힘듬)\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ0ovtnGZavB",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmYtxzoJWepO",
        "colab_type": "text"
      },
      "source": [
        "[참고자료]\n",
        "\n",
        "- https://gomguard.tistory.com/86\n",
        "- https://ratsgo.github.io/machine%20learning/2017/07/08/treecode/\n",
        "- https://www.youtube.com/watch?v=lIT5-piVtRw"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do5jVKOlkV7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forest_classify(trees, input):\n",
        "    votes = [classify(tree, input) for tree in trees]\n",
        "    vote_counts = Counter(votes)\n",
        "    return vote_counts.most_common(1)[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bROMSc9EeAtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def entropy(class_probabilities):\n",
        "    \"\"\"클래스에 속할 확률을 입력하면 엔트로피를 계산하라\"\"\"\n",
        "    return sum(-p * math.log(p,2)\n",
        "    for p in class_probabilities if p)\n",
        "\n",
        "def class_probabilities(labels):\n",
        "    total_count = len(labels)\n",
        "    return [count / total_count for count in Counter(labels).values()]\n",
        "\n",
        "def data_entropy(labeled_data):\n",
        "    labels = [label for _, label in labeled_data]\n",
        "    probabilities = class_probabilities(labels)\n",
        "    return entropy(probabilities)\n",
        "\n",
        "def partition_entropy(subsets):\n",
        "    total_count = sum(len(subset) for subset in subsets)\n",
        "    return sum( data_entropy(subset) * len(subset / total_count\n",
        "                                           for subset in subsets))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ut3gv_ygwP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import List\n",
        "import math\n",
        "\n",
        "def entropy(class_probabilities: List[float]) -> float:\n",
        "    \"\"\"Given a list of class probabilities, compute the entropy\"\"\"\n",
        "    return sum(-p * math.log(p, 2)\n",
        "               for p in class_probabilities\n",
        "               if p > 0)                     # ignore zero probabilities\n",
        "\n",
        "assert entropy([1.0]) == 0\n",
        "assert entropy([0.5, 0.5]) == 1\n",
        "assert 0.81 < entropy([0.25, 0.75]) < 0.82\n",
        "\n",
        "from typing import Any\n",
        "from collections import Counter\n",
        "\n",
        "def class_probabilities(labels: List[Any]) -> List[float]:\n",
        "    total_count = len(labels)\n",
        "    return [count / total_count\n",
        "            for count in Counter(labels).values()]\n",
        "\n",
        "def data_entropy(labels: List[Any]) -> float:\n",
        "    return entropy(class_probabilities(labels))\n",
        "\n",
        "assert data_entropy(['a']) == 0\n",
        "assert data_entropy([True, False]) == 1\n",
        "assert data_entropy([3, 4, 4, 4]) == entropy([0.25, 0.75])\n",
        "\n",
        "def partition_entropy(subsets: List[List[Any]]) -> float:\n",
        "    \"\"\"Returns the entropy from this partition of data into subsets\"\"\"\n",
        "    total_count = sum(len(subset) for subset in subsets)\n",
        "\n",
        "    return sum(data_entropy(subset) * len(subset) / total_count\n",
        "               for subset in subsets)\n",
        "\n",
        "from typing import NamedTuple, Optional\n",
        "\n",
        "class Candidate(NamedTuple):\n",
        "    level: str\n",
        "    lang: str\n",
        "    tweets: bool\n",
        "    phd: bool\n",
        "    did_well: Optional[bool] = None  # allow unlabeled data\n",
        "\n",
        "                  #  level     lang     tweets  phd  did_well\n",
        "inputs = [Candidate('Senior', 'Java',   False, False, False),\n",
        "          Candidate('Senior', 'Java',   False, True,  False),\n",
        "          Candidate('Mid',    'Python', False, False, True),\n",
        "          Candidate('Junior', 'Python', False, False, True),\n",
        "          Candidate('Junior', 'R',      True,  False, True),\n",
        "          Candidate('Junior', 'R',      True,  True,  False),\n",
        "          Candidate('Mid',    'R',      True,  True,  True),\n",
        "          Candidate('Senior', 'Python', False, False, False),\n",
        "          Candidate('Senior', 'R',      True,  False, True),\n",
        "          Candidate('Junior', 'Python', True,  False, True),\n",
        "          Candidate('Senior', 'Python', True,  True,  True),\n",
        "          Candidate('Mid',    'Python', False, True,  True),\n",
        "          Candidate('Mid',    'Java',   True,  False, True),\n",
        "          Candidate('Junior', 'Python', False, True,  False)\n",
        "         ]\n",
        "\n",
        "from typing import Dict, TypeVar\n",
        "from collections import defaultdict\n",
        "\n",
        "T = TypeVar('T')  # generic type for inputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bwhXXS1idxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter, defaultdict\n",
        "from functools import partial\n",
        "import math, random\n",
        "\n",
        "def entropy(class_probabilities):\n",
        "    \"\"\"given a list of class probabilities, compute the entropy\"\"\"\n",
        "    return sum(-p * math.log(p, 2) for p in class_probabilities if p)\n",
        "\n",
        "def class_probabilities(labels):\n",
        "    total_count = len(labels)\n",
        "    return [count / total_count\n",
        "            for count in Counter(labels).values()]\n",
        "\n",
        "def data_entropy(labeled_data):\n",
        "    labels = [label for _, label in labeled_data]\n",
        "    probabilities = class_probabilities(labels)\n",
        "    return entropy(probabilities)\n",
        "\n",
        "def partition_entropy(subsets):\n",
        "    \"\"\"find the entropy from this partition of data into subsets\"\"\"\n",
        "    total_count = sum(len(subset) for subset in subsets)\n",
        "\n",
        "    return sum( data_entropy(subset) * len(subset) / total_count\n",
        "                for subset in subsets )\n",
        "\n",
        "def group_by(items, key_fn):\n",
        "    \"\"\"returns a defaultdict(list), where each input item\n",
        "    is in the list whose key is key_fn(item)\"\"\"\n",
        "    groups = defaultdict(list)\n",
        "    for item in items:\n",
        "        key = key_fn(item)\n",
        "        groups[key].append(item)\n",
        "    return groups\n",
        "\n",
        "def partition_by(inputs, attribute):\n",
        "    \"\"\"returns a dict of inputs partitioned by the attribute\n",
        "    each input is a pair (attribute_dict, label)\"\"\"\n",
        "    return group_by(inputs, lambda x: x[0][attribute])\n",
        "\n",
        "def partition_entropy_by(inputs,attribute):\n",
        "    \"\"\"computes the entropy corresponding to the given partition\"\"\"\n",
        "    partitions = partition_by(inputs, attribute)\n",
        "    return partition_entropy(partitions.values())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLDxX-3LjkZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def classify(tree, input):\n",
        "    \"\"\"classify the input using the given decision tree\"\"\"\n",
        "\n",
        "    # if this is a leaf node, return its value\n",
        "    if tree in [True, False]:\n",
        "        return tree\n",
        "\n",
        "    # otherwise find the correct subtree\n",
        "    attribute, subtree_dict = tree\n",
        "\n",
        "    subtree_key = input.get(attribute)  # None if input is missing attribute\n",
        "\n",
        "    if subtree_key not in subtree_dict: # if no subtree for key,\n",
        "        subtree_key = None              # we'll use the None subtree\n",
        "\n",
        "    subtree = subtree_dict[subtree_key] # choose the appropriate subtree\n",
        "    return classify(subtree, input)     # and use it to classify the input\n",
        "\n",
        "def build_tree_id3(inputs, split_candidates=None):\n",
        "\n",
        "    # if this is our first pass,\n",
        "    # all keys of the first input are split candidates\n",
        "    if split_candidates is None:\n",
        "        split_candidates = inputs[0][0].keys()\n",
        "\n",
        "    # count Trues and Falses in the inputs\n",
        "    num_inputs = len(inputs)\n",
        "    num_trues = len([label for item, label in inputs if label])\n",
        "    num_falses = num_inputs - num_trues\n",
        "\n",
        "    if num_trues == 0:                  # if only Falses are left\n",
        "        return False                    # return a \"False\" leaf\n",
        "\n",
        "    if num_falses == 0:                 # if only Trues are left\n",
        "        return True                     # return a \"True\" leaf\n",
        "\n",
        "    if not split_candidates:            # if no split candidates left\n",
        "        return num_trues >= num_falses  # return the majority leaf\n",
        "\n",
        "    # otherwise, split on the best attribute\n",
        "    best_attribute = min(split_candidates,\n",
        "        key=partial(partition_entropy_by, inputs))\n",
        "\n",
        "    partitions = partition_by(inputs, best_attribute)\n",
        "    new_candidates = [a for a in split_candidates\n",
        "                      if a != best_attribute]\n",
        "\n",
        "    # recursively build the subtrees\n",
        "    subtrees = { attribute : build_tree_id3(subset, new_candidates)\n",
        "                 for attribute, subset in partitions.items() }\n",
        "\n",
        "    subtrees[None] = num_trues > num_falses # default case\n",
        "\n",
        "    return (best_attribute, subtrees)\n",
        "\n",
        "def forest_classify(trees, input):\n",
        "    votes = [classify(tree, input) for tree in trees]\n",
        "    vote_counts = Counter(votes)\n",
        "    return vote_counts.most_common(1)[0][0]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    inputs = [\n",
        "        ({'level':'Senior','lang':'Java','tweets':'no','phd':'no'},   False),\n",
        "        ({'level':'Senior','lang':'Java','tweets':'no','phd':'yes'},  False),\n",
        "        ({'level':'Mid','lang':'Python','tweets':'no','phd':'no'},     True),\n",
        "        ({'level':'Junior','lang':'Python','tweets':'no','phd':'no'},  True),\n",
        "        ({'level':'Junior','lang':'R','tweets':'yes','phd':'no'},      True),\n",
        "        ({'level':'Junior','lang':'R','tweets':'yes','phd':'yes'},    False),\n",
        "        ({'level':'Mid','lang':'R','tweets':'yes','phd':'yes'},        True),\n",
        "        ({'level':'Senior','lang':'Python','tweets':'no','phd':'no'}, False),\n",
        "        ({'level':'Senior','lang':'R','tweets':'yes','phd':'no'},      True),\n",
        "        ({'level':'Junior','lang':'Python','tweets':'yes','phd':'no'}, True),\n",
        "        ({'level':'Senior','lang':'Python','tweets':'yes','phd':'yes'},True),\n",
        "        ({'level':'Mid','lang':'Python','tweets':'no','phd':'yes'},    True),\n",
        "        ({'level':'Mid','lang':'Java','tweets':'yes','phd':'no'},      True),\n",
        "        ({'level':'Junior','lang':'Python','tweets':'no','phd':'yes'},False)\n",
        "    ]\n",
        "\n",
        "    for key in ['level','lang','tweets','phd']:\n",
        "        print(key, partition_entropy_by(inputs, key))\n",
        "    print()\n",
        "\n",
        "    senior_inputs = [(input, label)\n",
        "                     for input, label in inputs if input[\"level\"] == \"Senior\"]\n",
        "\n",
        "    for key in ['lang', 'tweets', 'phd']:\n",
        "        print(key, partition_entropy_by(senior_inputs, key))\n",
        "    print()\n",
        "\n",
        "    print(\"building the tree\")\n",
        "    tree = build_tree_id3(inputs)\n",
        "    print(tree)\n",
        "\n",
        "    print(\"Junior / Java / tweets / no phd\", classify(tree,\n",
        "        { \"level\" : \"Junior\",\n",
        "          \"lang\" : \"Java\",\n",
        "          \"tweets\" : \"yes\",\n",
        "          \"phd\" : \"no\"} ))\n",
        "\n",
        "    print(\"Junior / Java / tweets / phd\", classify(tree,\n",
        "        { \"level\" : \"Junior\",\n",
        "                 \"lang\" : \"Java\",\n",
        "                 \"tweets\" : \"yes\",\n",
        "                 \"phd\" : \"yes\"} ))\n",
        "\n",
        "    print(\"Intern\", classify(tree, { \"level\" : \"Intern\" } ))\n",
        "    print(\"Senior\", classify(tree, { \"level\" : \"Senior\" } ))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}