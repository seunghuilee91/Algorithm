{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA, Latent Dirichlet Allocation(잠재디리클레할당)",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNgNMkaQxrRatvnLjbuXxVG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghuilee91/Deeplearning/blob/master/LDA%2C_Latent_Dirichlet_Allocation(%EC%9E%A0%EC%9E%AC%EB%94%94%EB%A6%AC%ED%81%B4%EB%A0%88%ED%95%A0%EB%8B%B9).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0MdBBEUwC6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "import sys\n",
        "sys.path.append(\"/content/gdrive/My Drive/Colab Notebooks\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoZszhuPxttV",
        "colab_type": "text"
      },
      "source": [
        "# Latent Dirichlet Allocation, LDA\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- 말뭉치로부터 토픽을 추출하는 토픽모델링(Topic Modeling) \n",
        "\n",
        "- 텍스트 본문의 **숨겨진 의미 구조**를 발견하기 위해 사용\n",
        "\n",
        "- 토픽 모델링의 대표적인 알고리즘\n",
        "\n",
        " - 텍스트 데이터 소프트 클러스터링 \n",
        "\n",
        " - 텍스트 구조를 베이지안 네트워크 형태로 표현함 \n",
        "\n",
        "  - 베이지안 모델이면서 프라이어(priors)를 가짐\n",
        "\n",
        "- 활용 분야 : 주제 분류나 문서 간 유사도 계산 (검색 엔진, 고객 민원 시스템 등)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grb_Qjn4IMG7",
        "colab_type": "text"
      },
      "source": [
        "### 토픽 모델링이란?\n",
        "기계 학습 및 자연언어 처리 분야에서 토픽 모델(Topic model)이란 문서 집합의 추상적인 \"주제\"를 발견하기 위한 통계적 모델 중 하나로, 텍스트 본문의 숨겨진 의미구조를 발견하기 위해 사용되는 텍스트 마이닝 기법 중 하나이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3Oi8frSxtwS",
        "colab_type": "text"
      },
      "source": [
        "## 1. 모델 개요\n",
        "- LDA란 주어진 문서에 대하여 각 문서에 어떤 주제들이 존재하는지에 대한 확률모형\n",
        "- LDA는 토픽별 단어의 분포, 문서별 토픽의 분포를 모두 추정해 냅니다.\n",
        "<br><br>\n",
        "<br>\n",
        "\n",
        "\n",
        "### 1) LDA의 매커니즘\n",
        "<br>\n",
        "\n",
        "> [예제 문서] <BR>\n",
        ">문서1 : 저는 사과랑 바나나를 먹어요 <br>\n",
        ">문서2 : 우리는 귀여운 강아지가 좋아요  <br>\n",
        ">문서3 : 저의 깜찍하고 귀여운 강아지가 바나나를 먹어요\n",
        "\n",
        "<BR>\n",
        "\n",
        "**1. 세팅** <BR>\n",
        " 문서 집합에서 토픽이 몇 개가 존재할지 가정하는 것은 사용자가 정의 함. 이러한 하이퍼파라미터의 선택은 여러 실험을 통해 얻은 값일 수도 있고, 우선 시도해보는 값일 수도 있음.\n",
        "<BR>\n",
        "<BR>\n",
        "\n",
        "**2. 결과** <BR>\n",
        "각 문서의 토픽 분포와 각 토픽 내의 단어 분포를 추정\n",
        "<BR>\n",
        "<BR>\n",
        "<각 문서의 토픽 분포>  <br>\n",
        "문서1 : 토픽 A 100%  <br>\n",
        "문서2 : 토픽 B 100%  <br>\n",
        "문서3 : 토픽 B 60%, 토픽 A 40%  <br>\n",
        "\n",
        "<각 토픽의 단어 분포>  <br>\n",
        "토픽A : 사과 20%, 바나나 40%, 먹어요 40%, 귀여운 0%, 강아지 0%, 깜찍하고 0%, 좋아요 0%  <br>\n",
        "토픽B : 사과 0%, 바나나 0%, 먹어요 0%, 귀여운 33%, 강아지 33%, 깜찍하고 16%, 좋아요 16%  <br>\n",
        "<BR>\n",
        "\n",
        "**3. 정리** <BR>\n",
        "LDA는 토픽의 제목을 정해주지 않지만, 이 시점에서 알고리즘의 사용자는 위 결과로부터 두 토픽이 각각 과일에 대한 토픽과 강아지에 대한 토픽이라고 판단\n",
        "\n",
        "\n",
        "<BR>\n",
        "\n",
        "\n",
        "---\n",
        "<br><BR>\n",
        "\n",
        "### 2) LDA의 개략적인 도식\n",
        "\n",
        "<img src=\"https://i.imgur.com/r5e5qvs.png\">\n",
        "<br>\n",
        "\n",
        "> TOPICS : 우선 LDA는 특정 토픽에 특정 단어가 나타날 확률을 내어 준다.\n",
        "예컨대 위 그림에서 노란색 토픽엔 gene이라는 단어가 등장할 확률이 0.04, dna는 0.02, genetic은 0.01임. <BR>\n",
        "**즉,  이 노란색 토픽은 대략 ‘유전자’ 관련 주제라는 걸 알 수 있음**\n",
        "\n",
        "> TOPIC PROPORTIONS & ASSIGNMENTS : 주어진 문서를 보면 파란색, 빨간색 토픽에 해당하는 단어보다는 노란색 토픽에 해당하는 단어들이 많음. 따라서 위 문서의 메인 주제는 노란색 토픽(유전자 관련)일 가능성이 큼\n",
        "\n",
        "> LDA는 이렇게 말뭉치 이면에 존재하는 정보를 추론해낼 수 있습니다. LDA에 잠재(Latent)라는 이름이 붙은 이유입니다. LDA의 학습은 바로 이러한 **잠재정보를 알아내는 과정**입니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yuvrykYxty6",
        "colab_type": "text"
      },
      "source": [
        "## 2. 모델 아키텍처\n",
        "\n",
        "### 1) 아키텍처\n",
        "\n",
        "<img src=\"https://4four.us/assets/post_image/2010-11-07-latent-dirichlet-allocation-simply-1.png\">\n",
        "\n",
        "> α와 β는 코퍼스 단위로 정해지는 값 <br>\n",
        "> N과  는 문서 단위로 정해지는 값\n",
        "[N은 문서의 길이, θ는 해당 문서에서 각 주제의 가중치]\n",
        "<br>\n",
        "\n",
        "\n",
        "### 2) LDA의 수행\n",
        "\n",
        "**1) 사용자는 알고리즘에게 토픽의 개수 k를 알려준다.** <BR>\n",
        "LDA에게 토픽의 개수를 알려주는 역할은 사용자의 역할임. LDA는 토픽의 개수 k를 입력받으면, k개의 토픽이 M개의 전체 문서에 걸쳐 분포되어 있다고 가정함.\n",
        "\n",
        "**2) 모든 단어를 k개 중 하나의 토픽에 할당** <BR>\n",
        "LDA는 모든 문서의 모든 단어에 대해서 k개 중 하나의 토픽을 랜덤으로 할당함. 이 작업이 끝나면 각 문서는 토픽을 가지며, 토픽은 단어 분포를 가지는 상태가 됨. 물론 랜덤으로 할당하였기 때문에 사실 이 결과는 전부 틀린 상태임. (만약 한 단어가 한 문서에서 2회 이상 등장하였다면, 각 단어는 서로 다른 토픽에 할당되었을 수도 있음)\n",
        "\n",
        "**3) 이제 모든 문서의 모든 단어에 대해서 아래의 사항을 반복 진행. (iterative)**  <BR>\n",
        "\n",
        "**3-1) 어떤 문서의 각 단어 w는 자신은 잘못된 토픽에 할당되어져 있지만, 다른 단어들은 전부 올바른 토픽에 할당되어져 있는 상태라고 가정함. 이에 따라 단어 w는 아래의 두 가지 기준에 따라서 토픽이 재할당 됨**\n",
        "- p(topic t | document d) : 문서 d의 단어들 중 토픽 t에 해당하는 단어들의 비율\n",
        "- p(word w | topic t) : 단어 w를 갖고 있는 모든 문서들 중 토픽 t가 할당된 비율\n",
        "\n",
        "이를 반복하면, 모든 할당이 완료된 수렴 상태가 됨.\n",
        "\n",
        "\n",
        "<BR>\n",
        "\n",
        "**[예시]**\n",
        "\n",
        "<BR>\n",
        "\n",
        "(1) 두 개의 문서 doc1과 doc2를 보여줍니다. 여기서는 doc1의 세번째 단어 apple의 토픽을 결정하고자 합니다.\n",
        "\n",
        "<IMG SRC=\"https://wikidocs.net/images/page/30708/lda1.PNG\">\n",
        "\n",
        "<BR>\n",
        "<BR>\n",
        "\n",
        "(2) 우선 첫번째로 사용하는 기준은 문서 doc1의 단어들이 어떤 토픽에 해당하는지를 본다. doc1의 모든 단어들은 토픽 A와 토픽 B에 50 대 50의 비율로 할당되어져 있으므로, 이 기준에 따르면 단어 apple은 토픽 A 또는 토픽 B 둘 중 어디에도 속할 가능성이 있습니다.\n",
        "\n",
        "\n",
        "<IMG SRC=\"https://wikidocs.net/images/page/30708/lda3.PNG\">\n",
        "\n",
        "<BR>\n",
        "<BR>\n",
        "\n",
        "(3) 두번째 기준은 단어 apple이 전체 문서에서 어떤 토픽에 할당되어져 있는지를 봅니다. 이 기준에 따르면 단어 apple은 토픽 B에 할당될 가능성이 높습니다. 이러한 두 가지 기준을 참고하여 LDA는 doc1의 apple을 어떤 토픽에 할당할지 결정합니다.\n",
        "\n",
        "\n",
        "<IMG SRC=\"https://wikidocs.net/images/page/30708/lda2.PNG\">\n",
        "\n",
        "<BR>\n",
        "<BR>\n",
        "<BR>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<BR>\n",
        "\n",
        "\n",
        "\n",
        "이런 Generative Model을 써서 우리가 새로운 글을 쓰려는 건 당연히 아니다. 하지만, 이런 방법으로 문서 내용을 성공적으로 모델링, 즉 표현할 수 있다면 이미 존재하는 문서의 파라미터 θ를 찾아내는 것도 가능할 것이다. 글 d1과 d2가 있을 때, 주제는 비슷하더라도 각 문서에 등장하는 단어의 종류나 빈도는 다를 수 있기 때문에 단순한 키워드 기반의 모델로는 유사도를 계산하거나 주제 분류를 하는 데에 한계가 있다. 그러나 이미 보유한 많은 텍스트에 기초에 α와 β를 알아 두고, **개별 문서의 θ를 계산할 수 있다면, 이 θ를 가지고 유사도 계산이나 분류 작업을 훨씬 쉽고도 정확하게 해낼 수 있다**.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ilLecuVXRY2",
        "colab_type": "text"
      },
      "source": [
        "## 참고\n",
        "\n",
        "### 1) LDA가 가정하는 문서생성과정\n",
        "<BR><BR>\n",
        "<img src=\"https://4four.us/assets/post_image/2010-11-07-latent-dirichlet-allocation-simply-2.png\">\n",
        "<BR>\n",
        "<BR>\n",
        "\n",
        "\n",
        "### 2) LDA 이름의 의미\n",
        "\n",
        "- Latent:\n",
        "\n",
        "> 사전적인 의미는 “잠재적인, 숨어 있는”. 위에서 설명한 과정에서 우리가 직접 관찰할 수 있는 것은 문서 내용뿐이다. **α, β, θ, z는 모두 감춰진 파라미터**이다.\n",
        "\n",
        "- Dirichlet: \n",
        "\n",
        "> 19세기 독일 수학자의 이름. 디리클레 분포(Dirichlet Distribution)가 그의 이름을 따서 지어졌다고 한다. 제일 위의 코드를 보면 **θ를 결정할 때 α를 파라미터로 디리클레 분포**을 사용한다.\n",
        "\n",
        "- Allocation: \n",
        "\n",
        "> 말 그대로 ‘할당’. 각 단어를 결정할 때, **θ에 대한 다항 분포(Multinomial Distribution)로 주제를 ‘할당**’한 뒤 그 주제로부터 단어를 뽑는다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zxmbMDsde3x",
        "colab_type": "text"
      },
      "source": [
        "## 잠재 디리클레 할당(LDA) 실습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3rNtcvUZxoG",
        "colab_type": "text"
      },
      "source": [
        "### 1) 데이터 불러오기 & 정제하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37mHXfzHWBNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 사이킷런에서는 Twenty Newsgroups이라고 불리는 20개의 다른 주제를 가진 뉴스 데이터를 제공합니다. \n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data\n",
        "len(documents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bbigx040WZ_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 시작하기 앞서, 텍스트 데이터에 대해서 가능한한 정제 과정을 거쳐야만 합니다. 기본적인 아이디어는 알파벳을 제외한 구두점, 숫자, 특수 문자를 제거하는 것입니다. 이는 텍스트 전처리 챕터에서 정제 기법으로 배웠던 정규 표현식을 통해서 해결할 수 있습니다. 또한 짧은 단어는 유용한 정보를 담고있지 않다고 가정하고, 길이가 짧은 단어도 제거합니다. 그리고 마지막으로 모든 알파벳을 소문자로 바꿔서 단어의 개수를 줄이는 작업을 합니다.\n",
        "news_df = pd.DataFrame({'document':documents})\n",
        "# 특수 문자 제거\n",
        "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "# 전체 단어에 대한 소문자 변환\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMEScrXoWmEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_df['clean_doc'][1]\n",
        "\n",
        "# 우선 특수문자가 제거되었으며, if나 you와 같은 길이가 3이하인 단어가 제거된 것을 확인할 수 있습니다. 뿐만 아니라 대문자가 전부 소문자로 바뀌었습니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gowcD0D4XU97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mbyllkcWy1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  이제 뉴스 데이터에서 불용어를 제거합니다. 불용어를 제거하기 위해서 토큰화를 우선 수행합니다. 토큰화와 불용어 제거를 순차적으로 진행합니다.\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english') # NLTK로부터 불용어를 받아옵니다.\n",
        "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "# 불용어를 제거합니다.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKreYOhQWyNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 훈련용 뉴스를 5개만 출력해보겠습니다.\n",
        "tokenized_doc[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmXnPZSvYg6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  단어의 빈도수를 기록해보겠습니다. 여기서는 각 단어를 (word_id, word_frequency)\n",
        "\n",
        "from gensim import corpora\n",
        "dictionary = corpora.Dictionary(tokenized_doc)\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n",
        "print(corpus[1]) # 수행된 결과에서 두번째 뉴스 출력. 첫번째 문서의 인덱스는 0\n",
        "\n",
        "# (66, 2) 66번째 할당된 단어가 두번 등장함"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRJuYM_eZkVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(dictionary[66])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO03-OvFaKxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(dictionary)\n",
        "#  총 학습된 단어의 개수"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3WZksInZo7S",
        "colab_type": "text"
      },
      "source": [
        "### 2) LDA 모델 훈련시키기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rl3s5fcaT-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# 기존의 뉴스 데이터가 총 20개의 카테고리를 가지고 있었으므로 토픽의 개수를 20으로 하여 LDA 모델을 학습시켜보도록 하겠습니다.\n",
        "\n",
        "import gensim\n",
        "NUM_TOPICS = 20              #20개의 토픽, k=20\n",
        "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)  # passes는 알고리즘의 동작 횟수\n",
        "topics = ldamodel.print_topics(num_words=4)    # 총 4개의 단어만 출력\n",
        "for topic in topics:\n",
        "    print(topic)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "259BPjxkaAZw",
        "colab_type": "text"
      },
      "source": [
        "### 3) LDA 시각화 하기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGoL8TErZ_sq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install pyLDAvis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD5lMJT6aE52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyLDAvis.gensim\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
        "pyLDAvis.display(vis)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxrD1JmhaURR",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### 그래프 해석\n",
        "- 좌측의 원들은 각각의 20개의 토픽을 나타냄\n",
        "\n",
        "- 각 원과의 거리는 각 토픽들이 서로 얼마나 다른지를 보여 줌\n",
        "\n",
        "- 만약 두 개의 원이 겹친다면, 이 두 개의 토픽은 유사한 토픽이라는 의미\n",
        "\n",
        "- 위의 그림에서는 10번 토픽을 클릭하였고, 이에 따라 우측에는 10번 토픽에 대한 정보가 나타났음\n",
        "\n",
        "- 한 가지 주의할 점은 LDA 모델의 출력 결과에서는 토픽 번호가 0부터 할당되어 0-19의 숫자가 사용된 것과는 달리 위의 LDA 시각화에서는 토픽의 번호가 1부터 시작하므로 각 토픽 번호는 이제 +1이 된 값인 1-20까지의 값을 가집니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpQ3gQ8maT_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EXUZSMVbaLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, topic_list in enumerate(ldamodel[corpus]):\n",
        "    if i==5:\n",
        "        break\n",
        "    print(i,'번째 문서의 topic 비율은',topic_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS7PhP7zdLTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_topictable_per_doc(ldamodel, corpus):\n",
        "    topic_table = pd.DataFrame()\n",
        "\n",
        "    # 몇 번째 문서인지를 의미하는 문서 번호와 해당 문서의 토픽 비중을 한 줄씩 꺼내온다.\n",
        "    for i, topic_list in enumerate(ldamodel[corpus]):\n",
        "        doc = topic_list[0] if ldamodel.per_word_topics else topic_list            \n",
        "        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)\n",
        "        # 각 문서에 대해서 비중이 높은 토픽순으로 토픽을 정렬한다.\n",
        "        # EX) 정렬 전 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (10번 토픽, 5%), (12번 토픽, 21.5%), \n",
        "        # Ex) 정렬 후 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (12번 토픽, 21.5%), (10번 토픽, 5%)\n",
        "        # 48 > 25 > 21 > 5 순으로 정렬이 된 것.\n",
        "\n",
        "        # 모든 문서에 대해서 각각 아래를 수행\n",
        "        for j, (topic_num, prop_topic) in enumerate(doc): #  몇 번 토픽인지와 비중을 나눠서 저장한다.\n",
        "            if j == 0:  # 정렬을 한 상태이므로 가장 앞에 있는 것이 가장 비중이 높은 토픽\n",
        "                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)\n",
        "                # 가장 비중이 높은 토픽과, 가장 비중이 높은 토픽의 비중과, 전체 토픽의 비중을 저장한다.\n",
        "            else:\n",
        "                break\n",
        "    return(topic_table)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acViq2sddM_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topictable = make_topictable_per_doc(ldamodel, corpus)\n",
        "topictable = topictable.reset_index() # 문서 번호을 의미하는 열(column)로 사용하기 위해서 인덱스 열을 하나 더 만든다.\n",
        "topictable.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']\n",
        "topictable[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfjlrnHlYhHa",
        "colab_type": "text"
      },
      "source": [
        "reference\n",
        "\n",
        "https://wikidocs.net/30708"
      ]
    }
  ]
}