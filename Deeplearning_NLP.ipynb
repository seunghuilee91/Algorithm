{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deeplearning-NLP.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyOTSgmf9X4mQN8NrEjoi7bf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghuilee91/Deeplearning/blob/master/Deeplearning_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdRTXeIKhJAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "import sys\n",
        "sys.path.append(\"/content/gdrive/My Drive/Colab Notebooks\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqCLtLnhhOx6",
        "colab_type": "text"
      },
      "source": [
        "# 1. Bag of Words (BOW)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "  = 문자를 숫자로 표현하는 방법 중 한가지\n",
        "\n",
        "활용 : \n",
        "- 문장의 유사도를 알 수 있다(Sentence similarity)\n",
        "- 머신러닝 모델의 인풋데이터로 활용할 수 있다(Can be used as machin learning model's input data)\n",
        "- 쉽다\n",
        "\n",
        "단점 : \n",
        "- Sparsity : 실제 문장하나를 표현할 때 용량 메모리가 많이 들게 된다\n",
        "- Frequent words has more power \n",
        "- lgnoring word orders (home run vs run home)\n",
        "- Out of vocabulary (mis-spelling, unseen)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMyIuODLhPC6",
        "colab_type": "text"
      },
      "source": [
        "# 2. n-gram\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        " = 연속적인 n개의 토큰으로 구성된 것\n",
        "\n",
        "1) 1-gram(unigram) : 각각의 캐릭터가 하나의 토큰으로 구분됨 <br>\n",
        "2) 2-gram(bigram) : 두개의 캐릭터가 하나의 토큰으로 구분됨 \n",
        "etc..\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "## **Why n-gram?**\n",
        "- Overcome bag of words drawbacks (ingnoring sequence of words)\n",
        "- Next word prediction\n",
        "- Find misspelling\n",
        "- And more..\n",
        "\n",
        "\n",
        "### [Example]\n",
        "'Machine learning is fun and is not boring'\n",
        "\n",
        "**Bag of Words** : No way to know \"machine learning\" as one term and the word after \"not\". It's also translated as 'Machine is boring and learning is not fun' under BOA.\n",
        "\n",
        "**bigram** : \"machine learning\", \"not boring\" can be as token using it\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGZc6VT5hPQF",
        "colab_type": "text"
      },
      "source": [
        "# 3. TF-IDF\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        " = Term Frequency - Inverse Document Frequency <br>\n",
        " = To find how **relevant** a term is in a document <br>\n",
        "    * A word relevance is the amount of info that gives about tis context\n",
        "\n",
        "## What is Term Frequency?\n",
        " = TF measures how frequently a term occurs ina document <br>\n",
        " = (가정) If a term occurs more times than other terms in a document, the term has more relevance than otehr terms for the document\n",
        "\n",
        "### [Example]\n",
        "'A friend in need is a friend indeed'\n",
        "**TF** : 'a' is one term occurs frequently in any documents. Lee relevant to its document. We want to lower the measure if the term is just a frequent term to any document -> IDF\n",
        "\n",
        "## What is Inverse Document Frequency \n",
        " = Inverse document frequency <BR>\n",
        " = (IDF SCORE 공식) Log (Total # of docs / # of docs with the term in it +1)<BR>\n",
        "\n",
        " ### [Example]\n",
        "A : 'a new car, used car, car review' <br>\n",
        "B : 'A friend in need is a friend indeed'\n",
        "\n",
        "#### **[a]** \n",
        "[TF] <br>\n",
        "   A = 1/7    (7개 토큰중 1개가 속함)  <br>\n",
        "   B = 2/8    (8개 토큰중 2개가 속함)  <br>\n",
        "\n",
        "[IDF] <br>\n",
        "   Log (2/2) = 0     (2개 다큐먼트 중, 2개 다큐먼트에 다 포함됨)\n",
        "\n",
        "[TF-IDF] Score <br>\n",
        "A= 0, B= 0\n",
        "\n",
        "\n",
        "→ Car, Friend의 TF-IDF 값이 가장 큼\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "## **TF-IDF, BOA의 장점**\n",
        "- Easy to get document  similarity\n",
        "- Keep relevant words score\n",
        "- Lower just frequent words score\n",
        "\n",
        "## **TF-IDF, BOA의 단점**\n",
        "- Only based on Terms(words)\n",
        "- Weak on capturing document topic\n",
        "- Weak handling synonym (different word but same meaning)\n",
        "\n",
        "\n",
        "## **How to overcome Drawbacks of tf-idf?**\n",
        "- LSA (Latent Semantic Analysis)\n",
        "- Word Embeddings (Word2Vec, Glove)\n",
        "- ConceptNet\n",
        "\n",
        "\n",
        "   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXYaL5oIrn7E",
        "colab_type": "text"
      },
      "source": [
        "# 자연어처리 유사도 \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## How to check similarity in vector space?\n",
        " - Euclidean Distance (유클리디안 거리)\n",
        " - Cosine Similarity (코사인 유사도)\n",
        "\n",
        "<br>\n",
        "1. 유클리디안 거리 : 거리가 짧을면 유사도가 더 높다<br>\n",
        "2. 코사인 유사도 : 벡터의 크기(magnitude) 를 무시하고, 각도만 계산하다. <br>\n",
        " - money money money money money money <br>\n",
        " - money <br>\n",
        "→ money의 빈도수(크기) 때문에 오류가 생긴다. 이때 코사인 유사도를 사용하여 각도만 계산한다.\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CPjyNL5ro75",
        "colab_type": "text"
      },
      "source": [
        "# LSA (잠재 의미 분석)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "TF-IDF will have same zero similarity for pizza and hamburger since they **don't have same words**. Because TF-IDF or BoA similarity is based on **WORD**, but not on **TOPIC**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvN-b_1xro_y",
        "colab_type": "text"
      },
      "source": [
        "# Word2Vec\n",
        "\n",
        "---\n",
        "Can text be input in deep learning? NO! <BR>\n",
        "**Need to conver text to number (= Encodinig)**\n",
        "<br>\n",
        "<Br>\n",
        "## **One Hot Encoding?**\n",
        " '가장 사랑받는 인코딩 방법 <br>\n",
        " = Conver text to vector \n",
        "\n",
        "\n",
        "One Hot Encoding doesn't have similarity. So, need to use Embedding!\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "## **Embedding** \n",
        " = Embedding is dense vector with similarity\n",
        "\n",
        "좀 더 작은 차원의 벡터로 바꿔주며, 유사도를 구할 수 있다.\n",
        "\n",
        "<br>\n",
        "\n",
        "→ **원핫인코딩이 유사도를 구할 수 없다는 단점이 있기 때문에 임베딩을 사용하게 되었고, Word2Vec은 임베딩 방법이다.**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "# Word2Vec (비지도 학습)\n",
        "- Word embedding\n",
        "- Word similarity comes from the word's neighbor words\n",
        "- You also can esaily download pre-trained word2vec\n",
        "\n",
        "### **[Example]**\n",
        "A : \"King brave man\" <br>\n",
        "B : :queen  beautiful women\"\n",
        "\n",
        "**Word     Neighbor(target)** <br>\n",
        " king         brave<br>\n",
        " brave        king<br>\n",
        " brave        man<br>\n",
        " man          brave<br>\n",
        " queen       beautiful<br>\n",
        " beautiful   queen<br>\n",
        " beautiful   woman<br>\n",
        " woman       beautiful<br>\n",
        "[△ Train dtat generation with window size 1 (1개열)]\n",
        "\n",
        "'INPUT HIDDEN OUTPUT TARGET'\n",
        " = 이때 Hidden 값이 Word2Vec 임\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D04-qhmXrpDe",
        "colab_type": "text"
      },
      "source": [
        "# WMD (Word mover's distance)\n",
        " = Novel distance function between text documents <BR>\n",
        " = So you can get document similarity with WMD\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "## How WMD can give us document similarity?\n",
        " Using the Euclidean distance of Word2Vec\n",
        "\n",
        "\n",
        "## WMD document representation\n",
        "- Normalized Bag of Words after stop words removal\n",
        "- Normalized Bag of Words with value is greater than 0 (ex. President greets press Chicago  [0.25, 0.25, 0.25, 0.25]) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm_PUNuW-j5f",
        "colab_type": "text"
      },
      "source": [
        "# Tensor\n",
        "---\n",
        "\n",
        "**[Tensor example in NLP]**\n",
        "\n",
        "hi john / hi james / hi brian<br>\n",
        "word index one-hot-encoding vector<br>\n",
        "hi    0      [1,0,0,0]<br>\n",
        "john  1      [0,1,0,0]<br>\n",
        "james 2      [0,0,1,0]<br>\n",
        "brian 3      [0,0,0,1]<br>\n",
        "\n",
        "**sentence vector representation** <br>\n",
        "hi john  [[1,0,0,0], [0,1,0,0]]\n",
        "\n",
        "\n",
        "**mini batch input will be** <br>\n",
        "hi john hi james hi brian <br>\n",
        "[ [[1,0,0,0], [0,1,0,0]],[[1,0,0,0], [0,0,1,0]],[[1,0,0,0], [0,0,0,1]]] <br>\n",
        "\n",
        "(3) = 각 3개의 문장을 가지고 있다    *sample dimension\n",
        "\n",
        "(2) = 각 문장은 2개의 단어로 만들어짐   *max length of word\n",
        "\n",
        "(4) = 각 워드는 4개의 숫자로 표현이됨 * word vector dimension\n",
        "\n",
        "\n",
        " **(3,2,4) 3d tensor**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        " **tensor example in grayscale image = 3d tensor**\n",
        "\n",
        " (3,5,5)\n",
        " \n",
        " you have 3 images\n",
        "\n",
        " 5 rows\n",
        "\n",
        " 5 columns\n",
        "\n",
        " **tensor example in rgb color image = 4d tensor**\n",
        "\n",
        " (3,5,5,3)\n",
        " \n",
        " you have 3 images\n",
        "\n",
        " 5 rows\n",
        "\n",
        " 5 columns\n",
        " \n",
        " 3 colors (red, green, blue)\n",
        "\n",
        " **tensor example in rgb color image = 5d tensor**\n",
        "\n",
        " (3,5,5,5,3)\n",
        " \n",
        " you have 3 frames\n",
        "\n",
        " 5 images\n",
        "\n",
        " 5 rows\n",
        "\n",
        " 5 columns\n",
        " \n",
        " 3 colors (red, green, blue)\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD2tFA5L-j87",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saNOrIet-kAK",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBQq67_nroct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVOPtRByromB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNYMtkyLroq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}